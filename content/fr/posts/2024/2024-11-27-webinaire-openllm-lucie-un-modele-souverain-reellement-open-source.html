---
title: >-
  Webinaire OpenLLM - LUCIE, un modèle souverain réellement Open Source
subtitle: >-
  
url: "/actualites/2024-11-27-webinaire-openllm-lucie-un-modele-souverain-reellement-open-source/"
slug: "webinaire-openllm-lucie-un-modele-souverain-reellement-open-source"
date: 2024-11-27T00:00:00+01:00
lastmod: 2024-11-27T20:53:53+01:00
meta:
  hugo:
    permalink: "/actualites/2024-11-27-webinaire-openllm-lucie-un-modele-souverain-reellement-open-source/"
    file: "content/fr/posts/2024/2024-11-27-webinaire-openllm-lucie-un-modele-souverain-reellement-open-source.html"
    path: "webinaire-openllm-lucie-un-modele-souverain-reellement-open-source"
    slug: "webinaire-openllm-lucie-un-modele-souverain-reellement-open-source"
  dates:
    created_at: 2024-11-27T20:32:16+01:00
    updated_at: 2024-11-27T20:53:53+01:00
    published_at: 2024-11-27T00:00:00+01:00

breadcrumbs:
  - title: >-
      Accueil
    path: "/"
  - title: >-
      Actualités
    path: "/actualites/"
  - title: >-
      Webinaire OpenLLM - LUCIE, un modèle souverain réellement Open Source

design:
  full_width: false
  toc:
    present: false
    offcanvas: false

authors:
  - "clara-reig"
posts_categories:
  - "actualite-class-code"
  - "actualite"
  - "actualite-partenaires"
  - "thematiques-intelligence-articficielle"
taxonomies:
  - name: "Offre de formation"
    slug: "offre-de-formation"
    categories:

translationKey: communication-website-post-2adc306a-5984-4bfd-aac1-2dc6d7eb9c75

image:
  id: "db6c7924-2195-4d11-afc5-95c4518b963f"
  alt: ""
  credit: >-
    

shared_image:
  id: "50910a39-b694-4899-b019-420eb11b38f5"

meta_description: >-
  Tout comprendre de OpenLLM et LUCIE, un modèle souverain réellement Open Source

summary: >-
  

contents_reading_time:
  seconds: 516
  text: >-
    9 minutes
contents:
  - kind: block
    template: chapter
    title: >-
      
    slug: >-
      
    ranks:
      self: 2
    data:
      layout: no_background
      text: >-
        <p>Michel-Marie Maudet, Directeur Général et co-fondateur de LINAGORA, Bastien Masse, Délégué Général de l'Association ClassCode, et Olivier Gouvert, ingénieur de recherche chez LINAGORA, avaient donné rendez-vous ! </p><p>     Un moment très attendu par les membres de la communauté OpenLLM-France, les passionnés d’IA et d’Open Source, et tous les curieux désireux de connaître les avancées sur LUCIE : le tout premier modèle d’IA réellement Open Source, avec des données d’entrainement 100 % transparentes. </p><p>   Plus d'une centaine de personnes se sont connectés à ce webinaire démontrant le réel intérêt et la pertinence de ce projet pour le numérique d'aujourd'hui.</p><p> </p><p><b>        Ambitions de la communauté OpenLLM-France </b></p><p>Le modèle LUCIE est cours de développement par l’initiative OpenLLM-France, lancée en été 2023 pour promouvoir des modèles de langage ouverts (LLM) et souverains. Cette initiative regroupe une communauté de plus de 800 acteurs publics et privés francophones et européens, incluant chercheurs, startups, et entreprises. Le but est de créer des communs numériques pour l'IA générative, en développant un modèle de langage accessible à tous. </p><p> Ce projet a d’ailleurs répondu à un appel de l'État français visant à soutenir la création de modèles de langage souverains, orientés vers des cas d’usage comme celui de l’Éducation. Parmi les partenaires figure notamment l’Association Class’code, un acteur du secteur de l'éducation, ainsi que des institutions académiques et ministérielles. </p><p> Le modèle que développe OpenLLM-France est pensé pour être plus ouvert et éthique. Contrairement aux modèles propriétaires, comme ChatGPT, ce modèle sera entièrement open source (sous licence APACHE V2) et les données d’entraînement seront également accessibles en Creative Commons non commerciale, garantissant la transparence et le libre accès. </p><p> L’objectif technique est de produire un modèle léger, sobre en énergie et facile à déployer, pour permettre une adoption plus large dans l’éducation, malgré des contraintes budgétaires. </p><p> Michel-Marie MAUDET, Directeur Général de LINAGORA et co-initiateur du projet, explique l'ambition du projet : </p><p><i>  " Notre objectif c'est de développer un modèle de langage [...] sobre, compact, que l'on puisse utiliser assez simplement sans dépendre des grandes infrastructures que nous ne maîtrisons pas." </i></p><p>     En résumé, OpenLLM-France vise à créer un modèle de langage accessible et indépendant, adapté à des besoins spécifiques, en s'appuyant sur une collaboration étroite entre acteurs publics et privés.</p>

      notes: >-
        


      alt: >-
        

      credit: >-
        



  - kind: block
    template: image
    title: >-
      
    slug: >-
      
    ranks:
      self: 2
    data:
      text: >-
        

      image:
        id: "49575a3c-84b2-45b6-b959-368702b1d75a"
        file: "49575a3c-84b2-45b6-b959-368702b1d75a"

        alt: >-
          

        credit: >-
          



  - kind: block
    template: chapter
    title: >-
      LINAGORA s’allie à Classcode sous la bannière d’OpenLLM-France
    slug: >-
      linagora-s-allie-a-classcode-sous-la-banniere-d-openllm-france
    ranks:
      self: 2
    data:
      layout: no_background
      text: >-
        <p>Class'code est une initiative créée en 2015 pour former les enseignants à la culture du numérique et à la programmation, via des ressources éducatives libres. L’Association a déjà produit de nombreux e-books et un MOOC en IA, accessible gratuitement. Elle a aussi participé à des projets européens, comme AI4O, pour étudier l’impact de l'IA en éducation.</p><p>      Class'code travaille avec le GTNUM Génial, un groupe de travail soutenu par la Direction numérique pour l'Éducation et collaborant avec les académies de Versailles, Nantes et Marseille-Aix. L’objectif est de comprendre comment les enseignants et les élèves utilisent les IA génératives, d’identifier les défis d’évaluation, de protection des données, et d'approches pédagogiques. </p><p> Ainsi, l’initiative OpenLLM-France et Class’code répondent au besoin urgent d’outils opérationnels et éthiques pour exploiter l'IA en éducation, avec un LLM libre et un accompagnement de terrain pour former et outiller les enseignants. </p><p> Bastien MASSE, Délégué Général de Class'Code, explique </p><p><i>"Notre objectif, c'était de réunir autour de la table des personnes de la recherche, des universités et d'autres acteurs pour produire des ressources de haute qualité. "</i></p>

      notes: >-
        


      alt: >-
        

      credit: >-
        



  - kind: block
    template: chapter
    title: >-
      LUCIE, le modèle d’IA Open Source dédié à l’Éducation
    slug: >-
      lucie-le-modele-d-ia-open-source-dedie-a-l-education
    ranks:
      self: 2
    data:
      layout: no_background
      text: >-
        <p>Comme pour de nombreux secteurs, l’Éducation rencontre de nombreux défis liés à l’intégration de l’IA dans le parcours pédagogique des étudiants et enseignants, en particulier concernant le besoin d'acquérir des outils éducatifs fiables, respectueux des données et accessibles. Actuellement, il existe une multitude de solutions, mais peu sont adaptées aux exigences de l'éducation (stabilité, compatibilité avec la protection des données, documentation). </p><p> Lucie vise à répondre à ces besoins en offrant une bonne alternative, indépendante des grands acteurs commerciaux. Ce modèle est développé pour être stable et personnalisable, ce qui en fait un outil adapté aux EdTechs, enseignants, chercheurs et élèves. Il permettrait notamment des applications comme le RAG (Recherche Augmentée par l’IA) pour les enseignants, en utilisant des références fiables pour limiter les erreurs et les biais. Les enseignants pourraient ainsi intégrer leurs propres contenus de manière sécurisée et contrôlée. </p><p> Une version de petite taille du modèle (autour de 1 milliard de paramètres) sera également développée plus tard afin de permettre une utilisation du modèle en local sur des ordinateur de faible puissance. Rendant également possible un portage du modèle sur des devices de type Raspberry ou mini computer pour un usage sécurité sans réseau ni échange d’information serveur. </p><p> Le modèle LUCIE se distingue par sa sobriété, sa compacité et son respect de l'IACT, la réglementation européenne pour une IA de confiance. Conçu pour être entièrement Open Source, Lucie est libre d’utilisation, même commerciale, et tous les éléments (datasets, code) sont accessibles, permettant une transparence totale.</p>

      notes: >-
        


      alt: >-
        

      credit: >-
        



  - kind: block
    template: image
    title: >-
      
    slug: >-
      
    ranks:
      self: 2
    data:
      text: >-
        

      image:
        id: "a80d3d9d-f6db-4947-9234-b87d3e097372"
        file: "a80d3d9d-f6db-4947-9234-b87d3e097372"

        alt: >-
          

        credit: >-
          



  - kind: block
    template: chapter
    title: >-
      L’entrainement de LUCIE
    slug: >-
      l-entrainement-de-lucie
    ranks:
      self: 2
    data:
      layout: no_background
      text: >-
        <p>L’entraînement de ce modèle a commencé en août sur les infrastructures du CNRS : le supercalculateur Jean Zay, du GENCI, avec l’ambition de traiter 3 000 milliards de tokens pour lui fournir une vaste base de connaissances. Ce qui nécessité l'utilisation de 512 GPU H100 en parallèle et la collecte d'un jeu de donnée massif </p><p><i>" Sans trop divulguez les chiffre, on est sur 3000 milliards de tokens, on est sur de très grandes volumétries que l'on va mettre à disposition dans le cadre de cette démarche". </i> </p><p>Michel-Marie MAUDET </p><p>    1. Composition des données : Pour obtenir un modèle performant, Lucie est entraînée sur un mélange de langues et de types de données : anglais (33 % ), français (32,4 %), du code informatique et des mathématiques (15 %) car ils améliorent les capacités de raisonnement du modèle. Les autres langues comme l’allemand, l’espagnol et l’italien complètent les 20 % restants. Ainsi, chaque type de données est pondéré différemment pour prioriser la qualité (ex. : les données de Wikipédia sont vues plusieurs fois) et diversifier les compétences linguistiques du modèle. </p><p> 2. Filtrage des données : Afin d’assurer une qualité optimale, un filtrage strict est appliqué. Les données sont sélectionnées selon des critères de qualité pour éviter les informations redondantes, erronées ou offensantes. Par exemple, les URL de sites jugés offensants ou non pertinents sont exclus, et les contenus avec des mots bannis sont filtrés. Un processus de "near deduplication" est également employé pour éliminer les répétitions et garantir l’unicité des données. </p><p> 3. Ordonnancement des données et parallélisme d’entraînement : L’entraînement de Lucie se déroule en utilisant des GPU en parallèle et en appliquant une technique de parallélisme "3D" pour optimiser le processus. Les données plus anciennes sont traitées en début d’entraînement, ce qui permet de recentrer le modèle sur des informations actualisées en fin de parcours. Ce choix aide Lucie à maintenir des connaissances récentes. </p><p> Olivier GOUVERT, ingénieur de recherche chez LINAGORA, explique : </p><p><i>  "Nous avons moins de mal à accepter qu’il oublie des choses sur des données qui sont vieilles" </i></p><p>    4. Extension de la fenêtre de contexte : Lucie est en phase d'expansion de sa fenêtre contextuelle, de 4 096 à 128 000 tokens, permettant de mieux traiter les longs textes sans coupures. Cette capacité est cruciale pour des applications nécessitant une grande contextualisation, comme les systèmes de RAG. Par exemple, dans une application de questions-réponses pour une entreprise, cela permettrait au modèle de consulter un document d’entreprise entier, enrichissant ainsi la pertinence de ses réponses. </p><p> 5. Evaluation et comparaison des performances : Lucie est comparée à d’autres modèles comme Pythia, Croissant LLM et Falcon, sur des benchmarks français et anglais. Sa performance progresse de manière logarithmique, et les résultats montrent des prévisions prometteuses. À gauche du graphique, les benchmarks anglais montrent les étapes de progrès de Lucie par rapport à des modèles majoritairement anglophones, tandis que les benchmarks français soulignent sa compétence accrue dans cette langue. </p><p> Olivier GOUVERT souligne :   <i>" On voit que notre tokenizer est adapté pour ces 5 langues qu'on a choisi pour représenter un peu la diversité des langues européennes, ainsi que pour le code. "</i></p>

      notes: >-
        


      alt: >-
        

      credit: >-
        



  - kind: block
    template: chapter
    title: >-
      
    slug: >-
      
    ranks:
      self: 2
    data:
      layout: no_background
      text: >-
        <p>6. Phase d'instruction et d'alignement : Afin d'améliorer ses réponses et de les aligner sur les attentes humaines, Lucie passe par une phase d'instruction en deux étapes : </p><ul> <li> Fine-tuning (ajustement fin) : Cette première étape consiste à ajuster le modèle sur des données de type questions-réponses, plus proches de discussions humaines.     Par exemple, des jeux de données spécifiques sont créés avec des questions simples comme « Quelle est la capitale de la France ? », et des réponses précises sont fournies. Ce processus enseigne à Lucie qu’elle doit fournir une réponse directe, comme « Paris », explique Olivier. </li> <li>    Apprentissage par renforcement : Dans cette seconde étape, Lucie génère plusieurs réponses pour une même question, et celles-ci sont évaluées. Les préférences humaines sont ensuite intégrées pour affiner ses futures réponses.</li> </ul><p>Actuellement, le modèle Lucie est soigneusement entraîné avec une combinaison de données multilingues, un filtrage méticuleux et des stratégies avancées d'entraînement. Cela garantit un modèle final à la fois diversifié et performant pour des tâches de compréhension complexe et de génération de texte.</p>

      notes: >-
        


      alt: >-
        

      credit: >-
        



  - kind: block
    template: chapter
    title: >-
      La suite des événements...
    slug: >-
      la-suite-des-evenements
    ranks:
      self: 2
    data:
      layout: no_background
      text: >-
        <p>Michel-Marie MAUDET évoque les prochaines étapes : </p><p>  " On prévoit ça vraiment tout début décembre. Il y aura à la fois la publication du modèles, des data-sets, du code source et toute la sauce qu'on a utiliser pour entraîner le modèles. " </p><p>     Les équipes d’OpenLLM-France travaillent pour le moment à finaliser et à pré-instruire Lucie, en visant un modèle final d’ici fin novembre. La mise à disposition publique est prévue début décembre, avec une publication de toutes les ressources, comme le code source, les datasets, et les méthodes d'entraînement, en collaboration avec Hugging Face. </p><p> Lucie sera disponible sur cette plateforme, accompagnée d’une API gratuite pour les tests. Les utilisateurs pourront ainsi tester ou télécharger le modèle pour l’utiliser sur des outils locaux comme Ollama et LM Studio. </p><p>  Un accès sera également possible à travers la plateforme Vittascience, Edtech partenaire du projet, qui propose déjà des activités de médiation scientifiques sur l’IA. Permettant un accès pour les élèves et les enseignants sans nécessité de se connecter. </p><p>  LINAGORA prévoit aussi une plateforme SaaS avec un modèle d'abonnement pour utiliser Lucie, permettant de couvrir les coûts d’infrastructure, principalement dus aux GPU nécessaires. </p><p> Une version allégée du modèle à 1 milliard de paramètres est aussi en cours de préparation. Celui-ci vise une utilisation sur des appareils mobiles, y compris les téléphones, pour rendre l'accès aux grands modèles de langage possible dans un contexte local. </p><p> Toutes les informations, y compris le code source, seront disponibles sur le GitHub de la communauté : https://github.com/bentoml/OpenLLM</p>

      notes: >-
        


      alt: >-
        

      credit: >-
        



  - kind: block
    template: image
    title: >-
      
    slug: >-
      
    ranks:
      self: 2
    data:
      text: >-
        

      image:
        id: "d56aff57-0dc3-45c7-b5a3-90553a8240e6"
        file: "d56aff57-0dc3-45c7-b5a3-90553a8240e6"

        alt: >-
          

        credit: >-
          



  - kind: block
    template: image
    title: >-
      
    slug: >-
      
    ranks:
      self: 2
    data:
      text: >-
        

      image:
        id: "326c8988-4f09-4f58-8b02-76f6c33f8f36"
        file: "326c8988-4f09-4f58-8b02-76f6c33f8f36"

        alt: >-
          

        credit: >-
          



  - kind: block
    template: chapter
    title: >-
      
    slug: >-
      
    ranks:
      self: 2
    data:
      layout: no_background
      text: >-
        

      notes: >-
        


      alt: >-
        

      credit: >-
        




---

